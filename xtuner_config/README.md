# å¾®è°ƒæŒ‡å—

- æœ¬é¡¹ç›®ä¸ä»…åœ¨å¿ƒç†å¥åº·æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼ŒåŒæ—¶ä¹Ÿå¯¹æ¨¡å‹è¿›è¡Œäº†è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒï¼Œä¸‹é¢æ˜¯å¾®è°ƒçš„è¯¦ç»†æŒ‡å—ã€‚

## ä¸€ã€åŸºäºxtunerçš„å¾®è°ƒğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰

### ç¯å¢ƒå‡†å¤‡

```markdown
datasets==2.16.1
deepspeed==0.13.1
einops==0.7.0
flash_attn==2.5.0
mmengine==0.10.2
openxlab==0.0.34
peft==0.7.1
sentencepiece==0.1.99
torch==2.1.2
transformers==4.36.2
xtuner==0.1.11
```

ä¹Ÿå¯ä»¥ä¸€é”®å®‰è£…

```bash
cd xtuner_config/
pip3 install -r requirements.txt
```

---

### å¾®è°ƒ

```bash
cd xtuner_config/
xtuner train internlm2_7b_chat_qlora_e3.py --deepspeed deepspeed_zero2
```

---

### å°†å¾—åˆ°çš„ PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹

**å³ï¼šç”Ÿæˆ Adapter æ–‡ä»¶å¤¹**

```bash
cd xtuner_config/
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1

xtuner convert pth_to_hf internlm2_7b_chat_qlora_e3.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth ./hf
```

---

### å°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹

```bash
xtuner convert merge ./internlm2-chat-7b ./hf ./merged --max-shard-size 2GB
# xtuner convert merge \
#     ${NAME_OR_PATH_TO_LLM} \
#     ${NAME_OR_PATH_TO_ADAPTER} \
#     ${SAVE_PATH} \
#     --max-shard-size 2GB
```

---

### æµ‹è¯•

```
cd demo/
python cli_internlm2.py
```

---

## äºŒã€åŸºäºTransformersçš„å¾®è°ƒğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰

- è¯·æŸ¥çœ‹[ChatGLM3-6b loraå¾®è°ƒæŒ‡å—](ChatGLM3-6b-ft.md)

---

## å…¶ä»–

æ¬¢è¿å¤§å®¶ç»™[xtuner](https://github.com/InternLM/xtuner)å’Œ[EmoLLM](https://github.com/aJupyter/EmoLLM)ç‚¹ç‚¹star~

ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰
