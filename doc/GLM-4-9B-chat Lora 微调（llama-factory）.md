# GLM4-9B-chat Lora å¾®è°ƒ.

ä»‹ç»å¦‚ä½•åŸºäº llama-factory æ¡†æ¶ï¼Œå¯¹ glm-4-9b-chat æ¨¡å‹è¿›è¡Œ Lora å¾®è°ƒã€‚Lora æ˜¯ä¸€ç§é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ·±å…¥äº†è§£å…¶åŸç†å¯å‚è§åšå®¢ï¼š[çŸ¥ä¹|æ·±å…¥æµ…å‡º Lora](https://zhuanlan.zhihu.com/p/650197598)ã€‚

## ä¸€ã€ç¯å¢ƒå‡†å¤‡
æˆ‘ä»¬å®è·µäº†ä¸¤ç§å¹³å°è¿›è¡Œé€‰æ‹©
*  åœ¨[autodl](https://www.autodl.com/)å¹³å°ä¸­ç§Ÿä¸€ä¸ª3090ç­‰24Gæ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹©`PyTorch`-->`2.0.0`-->`3.8(ubuntu20.04)`-->`11.8`
![autodl](../xtuner_config/images/autodl.png)
  
  
*  åœ¨ [InternStudio](https://studio.intern-ai.org.cn/) å¹³å°ä¸­é€‰æ‹© A100(1/4) çš„é…ç½®ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© `Cuda11.7-conda`ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![internstudio](../xtuner_config/images/internstudio.png)
åœ¨Terminalä¸­ï¼Œè¿›è¡Œpipæ¢æºå’Œå®‰è£…ä¾èµ–åŒ…

## ç¯å¢ƒé…ç½®

åœ¨å®ŒæˆåŸºæœ¬ç¯å¢ƒé…ç½®å’Œæœ¬åœ°æ¨¡å‹éƒ¨ç½²çš„æƒ…å†µä¸‹ï¼Œä½ è¿˜éœ€è¦å®‰è£…ä¸€äº›ç¬¬ä¸‰æ–¹åº“ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# å®‰è£… LLaMA-Factory
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
#ä¸Šé¢è¿™æ­¥æ“ä½œä¼šå®Œæˆtorchã€transformersã€datasetsç­‰ç›¸å…³ä¾èµ–åŒ…çš„å®‰è£…

```


## äºŒã€æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ `modelscope` ä¸­çš„`snapshot_download`å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•°`cache_dir`ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `download.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ `python /root/autodl-tmp/download.py`æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 14 GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 10~20 åˆ†é’Ÿ

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('ZhipuAI/glm-4-9b-chat', cache_dir='/root/autodl-tmp', revision='master')
```

## ä¸‰ã€æŒ‡ä»¤é›†æ„å»º â€”â€” Alpaca æ ¼å¼

 LLaMA-Factory æ”¯æŒ alpaca æ ¼å¼å’Œ sharegpt æ ¼å¼çš„æ•°æ®é›†,æœ¬æ¬¡å¾®è°ƒæˆ‘ä»¬ä½¿ç”¨ alpaca æ ¼å¼

### æŒ‡ä»¤ç›‘ç£å¾®è°ƒæ•°æ®æ ¼å¼è¯´æ˜

åœ¨æŒ‡ä»¤ç›‘ç£å¾®è°ƒæ—¶ï¼Œ`instruction` åˆ—å¯¹åº”çš„å†…å®¹ä¼šä¸ `input` åˆ—å¯¹åº”çš„å†…å®¹æ‹¼æ¥åä½œä¸ºäººç±»æŒ‡ä»¤ï¼Œå³äººç±»æŒ‡ä»¤ä¸º `instruction\ninput`ã€‚è€Œ `output` åˆ—å¯¹åº”çš„å†…å®¹ä¸ºæ¨¡å‹å›ç­”ã€‚

å¦‚æœæŒ‡å®šï¼Œ`system` åˆ—å¯¹åº”çš„å†…å®¹å°†è¢«ä½œä¸ºç³»ç»Ÿæç¤ºè¯ã€‚

`history` åˆ—æ˜¯ç”±å¤šä¸ªå­—ç¬¦ä¸²äºŒå…ƒç»„æ„æˆçš„åˆ—è¡¨ï¼Œåˆ†åˆ«ä»£è¡¨å†å²æ¶ˆæ¯ä¸­æ¯è½®å¯¹è¯çš„æŒ‡ä»¤å’Œå›ç­”ã€‚æ³¨æ„åœ¨æŒ‡ä»¤ç›‘ç£å¾®è°ƒæ—¶ï¼Œå†å²æ¶ˆæ¯ä¸­çš„å›ç­”å†…å®¹**ä¹Ÿä¼šè¢«ç”¨äºæ¨¡å‹å­¦ä¹ **ã€‚

```json
[
  {
    "instruction": "äººç±»æŒ‡ä»¤ï¼ˆå¿…å¡«ï¼‰",
    "input": "äººç±»è¾“å…¥ï¼ˆé€‰å¡«ï¼‰",
    "output": "æ¨¡å‹å›ç­”ï¼ˆå¿…å¡«ï¼‰",
    "system": "ç³»ç»Ÿæç¤ºè¯ï¼ˆé€‰å¡«ï¼‰",
    "history": [
      ["ç¬¬ä¸€è½®æŒ‡ä»¤ï¼ˆé€‰å¡«ï¼‰", "ç¬¬ä¸€è½®å›ç­”ï¼ˆé€‰å¡«ï¼‰"],
      ["ç¬¬äºŒè½®æŒ‡ä»¤ï¼ˆé€‰å¡«ï¼‰", "ç¬¬äºŒè½®å›ç­”ï¼ˆé€‰å¡«ï¼‰"]
    ]
  }
]
```


### å•è½®å¯¹è¯æ•°æ®çš„æ ¼å¼è½¬æ¢
ä½¿ç”¨ä»¥ä¸‹ç¨‹åºå°†[æ•°æ®é›†](../datasets/)è½¬æ¢æˆ alpaca æ ¼å¼
```python
import json
import re

# é€‰æ‹©è¦æ ¼å¼è½¬æ¢çš„æ•°æ®é›†
file_name = "single_turn_dataset_1.json"
#file_name = "single_turn_dataset_2.json"

system_prompt = "å¦‚æœè¦æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼Œè¯·æ”¾åœ¨è¿™é‡Œ"

with open(f'../{file_name}', 'rt', encoding='utf-8') as file:
    data = json.load(file)

converted_data = [{"instruction": item["prompt"], 
                   "input": "", 
                   "output": item["completion"],
                   "system": system_prompt
                  } for item in data]

for i in range(len(converted_data)):

    # æ•°æ®æ¸…æ´—-å»æ‰ç‰¹æ®Šç¬¦å·
    if "ğŸ³" in converted_data[i]["output"]:
        converted_data[i]["output"] = converted_data[i]["output"].replace("ğŸ³", "")
        
    # æ•°æ®æ¸…æ´—-å»æ‰â€œä½ å¥½ï¼Œæˆ‘æ˜¯çº¢çƒ§è‚‰â€ï¼Œä¼šå½±å“å¤§æ¨¡å‹çš„è‡ªæˆ‘è®¤çŸ¥
    if 'å¥½ï¼Œæˆ‘æ˜¯' in converted_data[i]["output"]:
        converted_data[i]["output"] = converted_data[i]["output"].strip()
        intro_pattern = r"^[^\n]+\n"
        converted_data[i]["output"] = re.sub(intro_pattern, "", converted_data[i]["output"]).strip() 

with open(f'./processed/{file_name}', 'w', encoding='utf-8') as f:
    json.dump(converted_data, f, ensure_ascii=False, indent=4)
print(f'./processed/{file_name} Done')

```

### å¤šè½®å¯¹è¯æ•°æ®çš„æ ¼å¼è½¬æ¢
ä½¿ç”¨ä»¥ä¸‹ç¨‹åºå°†[æ•°æ®é›†](../datasets/)è½¬æ¢æˆ alpaca æ ¼å¼
```python
from tqdm import tqdm
import json

# é€‰æ‹©è¦æ ¼å¼è½¬æ¢çš„æ•°æ®é›†
file_name = "data.json"
#file_name = "data_pro.json"
#file_name = "multi_turn_dataset_1.json"
#file_name = "multi_turn_dataset_2.json"
#file_name = "aiwei.json"

system_prompt = "å¦‚æœè¦æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼Œè¯·æ”¾åœ¨è¿™é‡Œ"

with open(f'../{file_name}', 'rt', encoding='utf-8') as file:
    data = json.load(file)

# éå†åŸå§‹æ•°æ®ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢

# è½¬æ¢åçš„æ•°æ®æ ¼å¼
converted_data = []
for item in tqdm(data):
    conversation = item['conversation']
    history = [(c['input'], c['output']) for c in conversation[:-1]]
    last_item = conversation[-1]
    converted_data.append({
        "instruction": last_item['input'],
        "input": "",
        "output": last_item['output'],
        "system": system_prompt,
        "history": history
    })
    # å°†è½¬æ¢åçš„æ•°æ®è½¬æ¢ä¸ºJSONæ ¼å¼
    converted_json = json.dumps(converted_data, ensure_ascii=False, indent=4)

with open(f'./processed/{file_name}', 'w', encoding='utf-8') as f:
    json.dump(converted_data, f, ensure_ascii=False, indent=4)
```


### è§’è‰²æ‰®æ¼”æ•°æ®çš„æ ¼å¼è½¬æ¢
ä»£ç åŒä¸Šï¼Œæ ¹æ®åŸæ•°æ®é›†æ˜¯å•è½®å¯¹è¯è¿˜æ˜¯å¤šè½®å¯¹è¯æ¥é€‰æ‹©ã€‚æ³¨æ„è®¾ç½®å„ä¸ªè§’è‰²çš„â€œsystem_promptâ€ã€‚


### æ•°æ®é›†åˆå¹¶
ä¸ºäº†æ–¹ä¾¿å¤„ç†ï¼ˆä¸æƒ³åœ¨LLaMA-Factoryä¸­æ·»åŠ å¤ªå¤šçš„æ•°æ®é›†ï¼‰ï¼Œè¿™é‡Œå°†æ‰€æœ‰å·²ç»å¤„ç†å¥½çš„ alpaca æ ¼å¼çš„æ•°æ®é›†ï¼ˆæ¯ä¸€ä¸ªæ•°æ®é›†æ–‡ä»¶éƒ½æ˜¯ä¸€ä¸ªjsonå­—ç¬¦ä¸²ï¼‰åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼ˆä¸€ä¸ªå¤§çš„jsonå­—ç¬¦ä¸²ï¼‰ï¼Œåˆå¹¶ä»£ç å¦‚ä¸‹ï¼š
```python
import json

# åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨æ‰€æœ‰æ•°æ®
merged_data = []
file_list = [
    "single_turn_dataset_1.json",
    "single_turn_dataset_2.json",
    "self_cognition_EmoLLM.json",
    "ruozhiba_raw.json",
    "data.json",
    "data_pro.json",
    "multi_turn_dataset_1.json",
    "multi_turn_dataset_2.json",
    "aiwei.json",
    "tiangou.json",
    "SoulStar_data.json",
    "mother_v2.json",
    "scientist.json"
]

# éå†æ‰€æœ‰æ–‡ä»¶å¹¶è¯»å–æ•°æ®
for filename in file_list:
    with open(f"./processed/{filename}", 'r', encoding='utf-8') as file:
        data = json.load(file)
        merged_data.extend(data)

# å°†åˆå¹¶åçš„æ•°æ®å†™å…¥æ–°çš„ JSON æ–‡ä»¶
with open('emo_glm4_merged_data.json', 'w', encoding='utf-8') as output_file:
    json.dump(merged_data, output_file, ensure_ascii=False, indent=4)

print("åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜åˆ° emo_glm4_merged_data.json æ–‡ä»¶ä¸­ã€‚")
```


### å°†æ•°æ®é›†é…ç½®åˆ°LLaMA-Factory ä¸­

ä¿®æ”¹ LLaMa-Factory ç›®å½•ä¸­çš„ data/dataset_info.json æ–‡ä»¶ï¼Œåœ¨å…¶ä¸­æ·»åŠ ï¼š

```json
"emo_merged": {
  "file_name": "emo_glm4_merged_data.jsonæ–‡ä»¶çš„ç»å¯¹è·¯å¾„",
  }
}
```

## å››ã€å¾®è°ƒæ¨¡å‹
åœ¨ LLaMA-Factory ç›®å½•ä¸­æ–°å»ºé…ç½®æ–‡ä»¶ emo_glm4_lora_sft.yaml ï¼š
```python
### model
model_name_or_path: glm-4-9b-chatæ¨¡å‹åœ°å€çš„ç»å¯¹è·¯å¾„

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### dataset
# dataset è¦å’Œ data/dataset_info.json ä¸­æ·»åŠ çš„ä¿¡æ¯ä¿æŒä¸€è‡´
dataset: emo_merged
template: glm4
cutoff_len: 2048
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16

### output
# output_diræ˜¯æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„checkpointï¼Œè®­ç»ƒæ—¥å¿—ç­‰çš„ä¿å­˜ç›®å½•
output_dir: saves/emo-glm4-epoch10/lora/sft
logging_steps: 10
#save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_strategy: epoch

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 10.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
fp16: true

### eval
do_eval: false
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 10
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹å¾®è°ƒï¼š
```bash
cd LLaMA-Factory
llamafactory-cli train glm4_emo_lora_sft.yaml
```

è®­ç»ƒå®Œæˆåï¼Œåœ¨ LLaMA-Factory ç›®å½•ä¸­æ–°å»ºé…ç½®æ–‡ä»¶ emo_glm4_lora_sft_export.yaml:

```python
### model
model_name_or_path: glm-4-9b-chatæ¨¡å‹åœ°å€çš„ç»å¯¹è·¯å¾„
# åˆšæ‰emo_glm4_lora_sft.yamlæ–‡ä»¶ä¸­çš„ output_dir
adapter_name_or_path: saves/emo-glm4-epoch10/lora/sft
template: glm4
finetuning_type: lora

### export
export_dir: models/EmoLLM-glm-4-9b-chat
export_size: 2
export_device: cpu
export_legacy_format: false
```

## äº”ã€åˆå¹¶æ¨¡å‹

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹åˆå¹¶æ¨¡å‹ï¼š
```bash
cd LLaMA-Factory
llamafactory-cli export emo_glm4_lora_sft_export.yaml
```

åœ¨ models/EmoLLM-glm-4-9b-chat ç›®å½•ä¸­å°±å¯ä»¥è·å¾—ç»è¿‡Loraå¾®è°ƒåçš„å®Œæ•´æ¨¡å‹ã€‚

æ¨¡å‹æƒé‡å·²å¼€æºï¼š[ModelScope](https://www.modelscope.cn/models/wwewwt/EmoLLM-glm-4-9b-chat/summary)