# EmoLLM_Scientistå¾®è°ƒæŒ‡å—

## æ•°æ® 
å¾®è°ƒæ•°æ®å…±åŒ…å«3800æ®µå¯¹è¯ï¼Œå€ŸåŠ©LLMè‡ªåŠ¨ç”Ÿæˆï¼Œåç»­è¿›è¡Œäººå·¥æ ¡éªŒã€‚æ•°æ®è·¯å¾„ï¼š'datasets\scientist.json'

## åŸºåº§ 
åŸºåº§æ¨¡å‹é‡‡ç”¨InternLM2-Chat-7Bï¼Œæ¨¡å‹ä»‹ç»è¯·è§[InternLM](https://github.com/InternLM/InternLM)

## è®­ç»ƒæ–¹å¼ 
åŸºäºxtunerçš„å¾®è°ƒï¼Œä½¿ç”¨xtunerçš„trainå‘½ä»¤è¡Œå·¥å…·ï¼Œä½¿ç”¨å‘½ä»¤å¦‚ä¸‹ï¼š
### å®‰è£…ä¾èµ–

```bash
cd xtuner_config/
pip3 install -r requirements.txt
```

---
### è¿è¡Œå¾®è°ƒè„šæœ¬
```bash
cd xtuner_config/
xtuner train internlm2_7b_chat_qlora_e3_scienctist.py --deepspeed deepspeed_zero2
```

---
### æ¨¡å‹è½¬æ¢

å°†å¾—åˆ°çš„ PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼Œç”Ÿæˆ Adapter æ–‡ä»¶å¤¹

```bash
cd xtuner_config/
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1
#è¿™é‡Œå‡è®¾è®­ç»ƒäº†3ä¸ªepoch
xtuner convert pth_to_hf internlm2_7b_chat_qlora_e3_scienctist.py ./work_dirs/internlm2_7b_chat_qlora_e3_scienctist/epoch_3.pth ./hf
```

---

### å°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹

```bash
xtuner convert merge ./internlm2-chat-7b ./hf ./merged --max-shard-size 2GB
# xtuner convert merge \
#     ${NAME_OR_PATH_TO_LLM} \
#     ${NAME_OR_PATH_TO_ADAPTER} \
#     ${SAVE_PATH} \
#     --max-shard-size 2GB
```

---

### æµ‹è¯•

```
cd demo/
python cli_internlm2_scientist.py
```

---

## æ¨¡å‹ä¸Šä¼ 
å®Œæˆæµ‹è¯•åå¯å°†æ¨¡å‹ä¸Šä¼ åˆ°ModelScopeå’ŒOpenxlabå¹³å°
### ModelScope
è„šæœ¬ï¼š'scripts/upload_modelscope.py'
[Openxlabæ¨¡å‹ä¸Šä¼ ](https://openxlab.org.cn/docs/models/%E4%B8%8A%E4%BC%A0%E6%A8%A1%E5%9E%8B.html)

### Openxlab
[ModelScopeæ¨¡å‹ä¸Šä¼ ](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0)

## å…¶ä»–

æ¬¢è¿å¤§å®¶ç»™[xtuner](https://github.com/InternLM/xtuner)å’Œ[EmoLLM](https://github.com/aJupyter/EmoLLM)ç‚¹ç‚¹star~

ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰
